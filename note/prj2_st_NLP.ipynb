{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20230614\n",
    "* 챗봇 시스템\n",
    "  * 화장품쇼핑몰 사용자를 위한 챗봇시스템\n",
    "  * 연구실 구성원을 위한 챗봇시스템\n",
    "  * 컴퓨터용품 AS센터 챗봇시스템\n",
    "  * 사람이 타이핑한 글자를 인식\n",
    "\n",
    "* 프로그램: abc.exe\n",
    "* 프로세스: 프로그램이 동적인 메모리로 실행되어 있는 상태\n",
    "* 소멸자: 특정 인스턴스를 강제로 내릴때"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20230615\n",
    "* 데이터랭글링(필터링): 데이터 전처리\n",
    "* ```__dict__```: self, 인스턴스 변수에 접근"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20230619\n",
    "* 토크나이징\n",
    "  * 토큰: 문장에서 의미가 있는 가장작은 단위\n",
    "  * 문장을 토큰 단위로 나눔\n",
    "* 형태소 분석기\n",
    "  * 형태소: 언어의 가장작은 단위\n",
    "  * 형태소 분석기: 언어를 각종 품사 단위로 쪼개고, 단어와 품사를 태깅\n",
    "  * 단어를 어떻게 쪼개는냐에 따라 모델의 성능이 달라짐\n",
    "* 한국어 자연어 처리\n",
    "  * KoNLPy\n",
    "    * 오픈소스 라이브러리(konlpy.org/ko/latest)\n",
    "    * 세가지 형태소 분석기 모듈을 지원\n",
    "      * Kkma(꼬꼬마)\n",
    "        * morphs, nouns, pos, sentences\n",
    "      * Komoran(코모란)\n",
    "        * 자바로 개발한 한국어 형태소 분석기\n",
    "        * 공백이 포함된 형태소 단위로도 분석이 가능\n",
    "        * 사전관리하는 방법이 편리하고, 성능과 속도가 괜찮은 편\n",
    "        * morphs, nouns, pos\n",
    "      * Okt\n",
    "        * 트위터에서 개발한 Twitter 한국어 처리기에서 파생된 오픈소스\n",
    "        * 간단한 한국어 처리를 통해 색인어를 추출하는 목표(형태소 분석X)\n",
    "        * 분석되는 품사 정보는 작지만 속도는 제일 빠름\n",
    "        * normalize 함수를 통해 오타가 섞인 문장을 처리하는데 효과\n",
    "        * morphs, nouns, pos, normalize, phrases\n",
    "    * 설치방법: https://konlpy.org/ko/latest/install/#id2\n",
    "      * python 뿐만 아니라 java등의 다양한 언어 기반으로 개발되어 여러 의존성이 있음\n",
    "      * python 3.8대에서 호환이 좋음"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20230620\n",
    "#### 임베딩\n",
    "* 토크나이징 이후\n",
    "* 자연어를 숫자나 벡터 형태변환\n",
    "* 단어나 문장을 수치화해 벡터 공간으로 표현\n",
    "* 임베딩 유형\n",
    "  * 문장 임베딩: 개발 비용이 큼, 챗GPT\n",
    "  * 단어 임베딩: 간다한 챗봇, 동음이의어에 대한 구분을 못하는 단점\n",
    "    * 원핫인코딩: 단어사전의 단위행렬 적용, 유사한 단어와의 관계를 담고 있지 않아 활용도가 낮음.\n",
    "* 단위행렬, 대각행렬\n",
    "  * 대각행렬(diagonal matrix): 정사각 행렬 중 대각선의 값만 존재하는 행렬, 나머지는 0\n",
    "  * 단위행렬(identity matrix): 대각행렬중 주대각 행렬 값이 1인 행렬\n",
    "* 희소표현, 분산표현(밀집표현)\n",
    "  * 희소표현: \n",
    "    * 원핫인코딩\n",
    "    * 직관적\n",
    "    * 단어관 연관성을 담을 수 없음, 차원이 높아지기 쉬우며 이는 모델이 학습을 잘 못하게함(차원의 저주)\n",
    "  * 분산표현\n",
    "    * 임베딩 모델(임베딩을 위한 신경망등의 기법에 기반한 모델): CBOW, skip-gram\n",
    "      * CBOW: 주변 단어를 이용해 타겟 단어를 예측 --> 단순하여 빠름\n",
    "      * skip-gram: 입력 단어를 이용해 주변을 예측 --> 표현력이 더 좋음\n",
    "    * 비 지관적\n",
    "    * 단어간 유사성을 표현하기 위한 노력, 저차원으로 밀집(벡터 공간을 절약), 예) RGB(204,255,204)\n",
    "* word2vec\n",
    "  * \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20230621\n",
    "* FAQ, QNA 챗봇 개발 목표\n",
    "* 입력된 질문과, 시스템에 내재된 답변의 유사도를 이용\n",
    "* 단어 수치화\n",
    "  * 라벨인코딩, 원핫인코딩, word2vec(인공신경망)\n",
    "* 유사도\n",
    "  * 단어 또는 문장을 벡터화 한후 통계, 신경망 기법에 따라 유사도 계산\n",
    "  * 통계\n",
    "    * n-gram(문장 유사도)\n",
    "      * 문장을 n개의 토큰으로 분리\n",
    "      * 유사도 = 겹치는 토큰수 / 전체 토큰 수\n",
    "      * 기준 문장이 있어 기준에 따라 유사도 달라짐 \n",
    "      * 논문 도용등 방지 프로그램에 쓰임\n",
    "    * 코사인 유사도\n",
    "      * (1) 두문장의 명사 합집합을 각 문장별 카운트 행렬 생성\n",
    "      * (1)을 입력으로 두 카운트 행렬의 코사인값을 유사도로 생성(-1~1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20230622\n",
    "* 텐서플로우와 케라스의 역할 분담\n",
    "    * 텐서플로우\n",
    "      * 구글, 오픈소스\n",
    "      * 기계학습 라이브러리\n",
    "      * python, C++, Java 언어 지원\n",
    "    * "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20230626~\n",
    "* 6.1 숫자인식 인공신경망 개발\n",
    "  * 이미지 분류\n",
    "  * minist 필기 숫자\n",
    "  * 단순 신경망\n",
    "* 6.2 문장 의도 분류를 위한 CNN \n",
    "  * 문장 감정 분류\n",
    "  * 말뭉치\n",
    "  * 병렬 CNN\n",
    "  * max_seq_len은 문장 리스트중 가장 긴 문장의 길이로 보통\n",
    "* 6.3 개체명 인식을 위한 양방향 LSTM 모델\n",
    "  * 순차데이터를 입력으로 받아 원하는 개체를 출력하도록\n",
    "  * 개체는 단어일 수도 있고, 문장일 수도 있고 설정에 따라 다름"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20230629\n",
    "* 개체명 인식(NER; Named Entity Recognition)\n",
    "  * 챗봇에서 문장을 정확하게 해석하기 위한 전처리과정\n",
    "  * 문장의 단어가 \"인물\", \"장소\", \"날짜\"등과 같이 개체를 인식하는것\n",
    "  * text.txt\n",
    "    * ;변환전\n",
    "    * $ NER처리 된 후\n",
    "  * BIO 표기법(Beggining, Inside, Outside, 나머지 분류(B_OG, 등))\n",
    "    * B: 개체명이 시작되는 단어에 'B-개체명' 태그\n",
    "    * I: 'B-개체명' 태그와 연결되는 단어, 'I-개체명' 태그\n",
    "    * O: 개체명 외에 모든 단어 태그\n",
    "  * 데이터셋\n",
    "    * BIO 태그 데이터셋: 영어는 많음, 한글은 국립국어원 언어정보나눔터에 공개\n",
    "    * 책 데이터셋: HLCT(2016)를 수정한 KoreanNERCorpus\n",
    "      - github.com/machinereading/KoreanNERCorpus\n",
    "  * preprocessing.text.Tokenizer\n",
    "    * 라벨 인코딩\n",
    "    * Embedding 하기 전 전처리\n",
    "\n",
    "\n",
    "[질문] 예제 6-8코드 \n",
    "2. len(sent_tokenizer.word_index) + 1 --> \"+1\"하는 이유\n",
    "# 단어 사전 및 태그 사전 크기\n",
    "vocab_size = len(sent_tokenizer.word_index) + 1\n",
    " tag_size = len(tag_tokenizer.word_index) + 1\n",
    "\n",
    "2. sent_tokenizer = preprocessing.text.Tokenizer(oov_token='OOV'), # 첫번째 인덱스에는 OOV 사용 --> 코드가 하는 일이 뭔지 (OOV=out of vocabulary)\n",
    "-->\n",
    "\n",
    "3. 아래 코드 설명이 없음\n",
    "# index to word / index to NER 정의\n",
    "index_to_word = sent_tokenizer.index_word # 시퀀스 인덱스를 단어로 변환하기 위해 사용\n",
    "index_to_ner = tag_tokenizer.index_word # 시퀀스 인덱스를 NER로 변환하기 위해 사용\n",
    "index_to_ner[0] = 'PAD'\n",
    "--> \n",
    "\n",
    "4. 시퀀스 패딩 처리, max_len = 40  # 책에서는 평균길이 36보다 조금 큰 값으로 설정 --> 그러면 최대길이 168은 인식을 안하기로 한거?\n",
    "-->\n",
    "\n",
    "5. Embedding layer 설정\n",
    "5.1 입력벡터의 차원은 40인데 출력차원은 30으로 정한이유, 앞서 배운 CNN에서는 15를 받아서 128로 했었음, 설정의 차이가 무엇인가?\n",
    "5.2 책에서 앞선 내용에서 다설명해서 새로울게 없다고 했는데, Mask=True 해놓고 설명이 없음\n",
    "--> \n",
    "\n",
    "6. f1-score error, B_DT, B_PS, B_OG, B_TI, B_LC 오류나서 알 수 없음\n",
    "c:\\Python38\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: B_DT seems not to be NE tag.\n",
    "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
    "c:\\Python38\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: B_PS seems not to be NE tag.\n",
    "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
    "c:\\Python38\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: B_OG seems not to be NE tag.\n",
    "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
    "c:\\Python38\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: B_TI seems not to be NE tag.\n",
    "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
    "c:\\Python38\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: B_LC seems not to be NE tag.\n",
    "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
    "\n",
    "7. 아래 코드에서 새로운 문장에 대한 BIO 코드 정답이 없어서 맞았는지 안맞았는지 모르는데, 그냥 예측해보는것에 의미를 두면 되는 것인지\n",
    "# 출력\n",
    "# \"입력= 삼성전자 출시 스마트폰 오늘 애플 도전장 내밀다.\"\n",
    "print(\"{:10} {:5}\".format(\"단어\", \"예측된 NER\"))\n",
    "print(\"-\" * 50)\n",
    "for w, pred in zip(new_sentence, p[0]):\n",
    "    print(\"{:10} {:5}\".format(w, index_to_ner[pred]))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
