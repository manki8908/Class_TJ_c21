{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[질문]\n",
    "1. \"text.Tokenizer()\" 토큰화라는 명칭을 함수에서 임베딩을 하고 있음 --> 책에서 배운 \"토큰화, 임베딩\" 개념하고 달라 혼란스러움\n",
    "- 책에서 말하는 토큰화를 여기서는 \"preprocessing.text.text_to_word_sequence\"로 음절 분리를 하고 있음\n",
    "- 책에서 말하는 임베딩을 여기서는 \"text.Tokenizer()\"를 통해 라벨 인코딩을 하고 있음\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus[0:10] [['12시', '땡'], ['1지망', '학교', '떨어졌어'], ['3박4일', '놀러가고', '싶다'], ['3박4일', '정도', '놀러가고', '싶다'], ['ppl', '심하네'], ['sd카드', '망가졌어'], ['sd카드', '안돼'], ['sns', '맞팔', '왜', '안하지ㅠㅠ'], ['sns', '시간낭비인', '거', '아는데', '매일', '하는', '중'], ['sns', '시간낭비인데', '자꾸', '보게됨']]\n",
      "<class 'list'>\n",
      "sequences[0:10] [[4646, 4647], [4648, 343, 448], [2580, 803, 11], [2580, 804, 803, 11], [4649, 2581], [2582, 4650], [2582, 64], [805, 4651, 14, 4652], [805, 4653, 3, 502, 238, 45, 106], [805, 4654, 23, 4655]]\n",
      "<class 'numpy.ndarray'>\n",
      "[[4646 4647    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [4648  343  448    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2580  803   11    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2580  804  803   11    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [4649 2581    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2582 4650    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2582   64    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 805 4651   14 4652    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 805 4653    3  502  238   45  106    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 805 4654   23 4655    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n"
     ]
    }
   ],
   "source": [
    "# 6-3\n",
    "\n",
    "# 필요한 모듈 임포트\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Conv1D, GlobalMaxPool1D, concatenate\n",
    "\n",
    "\n",
    "# 데이터 읽어오기\n",
    "train_file = \"../../../chatbot_book_ex/book_ex/ch6/chatbot_data.csv\"\n",
    "data = pd.read_csv(train_file, delimiter=',')\n",
    "features = data['Q'].tolist()\n",
    "labels = data['label'].tolist()\n",
    "\n",
    "\n",
    "# 1. 음절단위 분류  2. 토크나이저 핏 3. 토크나이저에서 시퀀스 추출  4. 3을 패딩\n",
    "\n",
    "# 단어 인덱스 시퀀스 벡터, 음절단위 분류\n",
    "corpus = [preprocessing.text.text_to_word_sequence(text) for text in features]\n",
    "print(\"corpus[0:10]\", corpus[0:10])\n",
    "\n",
    "\n",
    "# 단어 토큰화??, 음절단위 분리 후 또하는 이유는?\n",
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "\n",
    "# 임베딩, 라벨 인코더\n",
    "sequences = tokenizer.texts_to_sequences(corpus)  # 총 단어에 각기 다른 숫자를 부여한 리스트 반환 --> 임베딩의 한 방법중 라벨 인코딩 같음\n",
    "word_index = tokenizer.word_index\n",
    "print(type(sequences))\n",
    "print(\"sequences[0:10]\", sequences[0:10])\n",
    "\n",
    "\n",
    "# 15개 단어 차원으로 패딩이 들어감\n",
    "MAX_SEQ_LEN = 15  # 단어 시퀀스 벡터 크기\n",
    "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')\n",
    "print(type(padded_seqs))\n",
    "print(padded_seqs[0:10])\n",
    "\n",
    "\n",
    "# # 학습용, 검증용, 테스트용 데이터셋 생성 ➌\n",
    "# # 학습셋:검증셋:테스트셋 = 7:2:1\n",
    "# ds = tf.data.Dataset.from_tensor_slices((padded_seqs, labels))\n",
    "# ds = ds.shuffle(len(features))\n",
    "\n",
    "# train_size = int(len(padded_seqs) * 0.7)\n",
    "# val_size = int(len(padded_seqs) * 0.2)\n",
    "# test_size = int(len(padded_seqs) * 0.1)\n",
    "\n",
    "# train_ds = ds.take(train_size).batch(20)\n",
    "# val_ds = ds.skip(train_size).take(val_size).batch(20)\n",
    "# test_ds = ds.skip(train_size + val_size).take(test_size).batch(20)\n",
    "\n",
    "# # 하이퍼파라미터 설정\n",
    "# dropout_prob = 0.5\n",
    "# EMB_SIZE = 128\n",
    "# EPOCH = 5\n",
    "# VOCAB_SIZE = len(word_index) + 1  # 전체 단어 수\n",
    "\n",
    "# # CNN 모델 정의\n",
    "# input_layer = Input(shape=(MAX_SEQ_LEN,))\n",
    "# embedding_layer = Embedding(VOCAB_SIZE, EMB_SIZE, input_length=MAX_SEQ_LEN)(input_layer)\n",
    "# dropout_emb = Dropout(rate=dropout_prob)(embedding_layer)\n",
    "\n",
    "# conv1 = Conv1D(filters=128, kernel_size=3, padding='valid', activation=tf.nn.relu)(dropout_emb)\n",
    "# pool1 = GlobalMaxPool1D()(conv1)\n",
    "# conv2 = Conv1D(filters=128, kernel_size=4, padding='valid', activation=tf.nn.relu)(dropout_emb)\n",
    "# pool2 = GlobalMaxPool1D()(conv2)\n",
    "# conv3 = Conv1D(filters=128, kernel_size=5, padding='valid', activation=tf.nn.relu)(dropout_emb)\n",
    "# pool3 = GlobalMaxPool1D()(conv3)\n",
    "\n",
    "# # 3, 4, 5- gram 이후 합치기\n",
    "# concat = concatenate([pool1, pool2, pool3])\n",
    "# hidden = Dense(128, activation=tf.nn.relu)(concat)\n",
    "# dropout_hidden = Dropout(rate=dropout_prob)(hidden)\n",
    "# logits = Dense(3, name='logits')(dropout_hidden)\n",
    "# predictions = Dense(3, activation=tf.nn.softmax)(logits)\n",
    "\n",
    "# # 모델 생성\n",
    "# model = Model(inputs=input_layer, outputs=predictions)\n",
    "# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # 모델 학습\n",
    "# model.fit(train_ds, validation_data=val_ds, epochs=EPOCH, verbose=1)\n",
    "\n",
    "# # 모델 평가(테스트 데이터셋 이용)\n",
    "# loss, accuracy = model.evaluate(test_ds, verbose=1)\n",
    "# print('Accuracy: %f' % (accuracy * 100))\n",
    "# print('loss: %f' % (loss))\n",
    "\n",
    "# # 모델 저장\n",
    "# model.save('../MODL/cnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 15)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 15, 128)      1715072     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 15, 128)      0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 13, 128)      49280       ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 12, 128)      65664       ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 11, 128)      82048       ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " global_max_pooling1d (GlobalMa  (None, 128)         0           ['conv1d[0][0]']                 \n",
      " xPooling1D)                                                                                      \n",
      "                                                                                                  \n",
      " global_max_pooling1d_1 (Global  (None, 128)         0           ['conv1d_1[0][0]']               \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " global_max_pooling1d_2 (Global  (None, 128)         0           ['conv1d_2[0][0]']               \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 384)          0           ['global_max_pooling1d[0][0]',   \n",
      "                                                                  'global_max_pooling1d_1[0][0]', \n",
      "                                                                  'global_max_pooling1d_2[0][0]'] \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          49280       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 128)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " logits (Dense)                 (None, 3)            387         ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 3)            12          ['logits[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,961,743\n",
      "Trainable params: 1,961,743\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "100/100 - 0s - loss: 0.0667 - accuracy: 0.9820 - 304ms/epoch - 3ms/step\n",
      "단어 시퀀스 :  ['썸', '타는', '여자가', '남사친', '만나러', '간다는데', '뭐라', '해']\n",
      "단어 인덱스 시퀀스 :  [   13    61   127  4320  1333 12162   856    31     0     0     0     0\n",
      "     0     0     0]\n",
      "문장 분류(정답) :  2\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "감정 예측 점수 :  [[1.2253551e-07 1.2767056e-06 9.9999857e-01]]\n",
      "감정 예측 클래스 :  [2]\n"
     ]
    }
   ],
   "source": [
    "# 6-4\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras import preprocessing\n",
    "\n",
    "# 데이터 읽어오기\n",
    "train_file = \"../../chatbot/book_ex/ch6/chatbot_data.csv\"\n",
    "data = pd.read_csv(train_file, delimiter=',')\n",
    "features = data['Q'].tolist()\n",
    "labels = data['label'].tolist()\n",
    "\n",
    "# 단어 인덱스 시퀀스 벡터\n",
    "corpus = [preprocessing.text.text_to_word_sequence(text) for text in features]\n",
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "MAX_SEQ_LEN = 15 # 단어 시퀀스 벡터 크기\n",
    "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')\n",
    "\n",
    "# 테스트용 데이터셋 생성\n",
    "# ??? 위에서 정의한 test_ds 쓰지 않고 ds.take(2000)을 쓴 이유\n",
    "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, labels))\n",
    "ds = ds.shuffle(len(features))\n",
    "test_ds = ds.take(2000).batch(20) # 테스트 데이터셋\n",
    "\n",
    "# 감정 분류 CNN 모델 불러오기\n",
    "model = load_model('../MODL/cnn_model.h5')\n",
    "model.summary()\n",
    "model.evaluate(test_ds, verbose=2)   # loss & acc\n",
    "\n",
    "# 테스트용 데이터셋의 10212번째 데이터 출력\n",
    "print(\"단어 시퀀스 : \", corpus[10212])\n",
    "print(\"단어 인덱스 시퀀스 : \", padded_seqs[10212])\n",
    "print(\"문장 분류(정답) : \", labels[10212])\n",
    "\n",
    "# 테스트용 데이터셋의 10212번째 데이터 감정 예측\n",
    "picks = [10212]\n",
    "predict = model.predict(padded_seqs[picks])\n",
    "predict_class = tf.math.argmax(predict, axis=1)\n",
    "print(\"감정 예측 점수 : \", predict)\n",
    "print(\"감정 예측 클래스 : \", predict_class.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[[4646, 4647], [4648, 343, 448], [2580, 803, 11], [2580, 804, 803, 11], [4649, 2581], [2582, 4650], [2582, 64], [805, 4651, 14, 4652], [805, 4653, 3, 502, 238, 45, 106], [805, 4654, 23, 4655]]\n",
      "<class 'numpy.ndarray'>\n",
      "[[4646 4647    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [4648  343  448    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2580  803   11    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2580  804  803   11    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [4649 2581    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2582 4650    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [2582   64    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 805 4651   14 4652    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 805 4653    3  502  238   45  106    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [ 805 4654   23 4655    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "Epoch 1/7\n",
      "414/414 [==============================] - 44s 103ms/step - loss: 0.8653 - accuracy: 0.5874 - val_loss: 0.4922 - val_accuracy: 0.8147\n",
      "Epoch 2/7\n",
      "414/414 [==============================] - 41s 100ms/step - loss: 0.4788 - accuracy: 0.8259 - val_loss: 0.2330 - val_accuracy: 0.9230\n",
      "Epoch 3/7\n",
      "414/414 [==============================] - 41s 100ms/step - loss: 0.2817 - accuracy: 0.9072 - val_loss: 0.1606 - val_accuracy: 0.9505\n",
      "Epoch 4/7\n",
      "414/414 [==============================] - 42s 101ms/step - loss: 0.1616 - accuracy: 0.9502 - val_loss: 0.0768 - val_accuracy: 0.9755\n",
      "Epoch 5/7\n",
      "414/414 [==============================] - 42s 101ms/step - loss: 0.1157 - accuracy: 0.9675 - val_loss: 0.0655 - val_accuracy: 0.9772\n",
      "Epoch 6/7\n",
      "414/414 [==============================] - 43s 103ms/step - loss: 0.0865 - accuracy: 0.9728 - val_loss: 0.0442 - val_accuracy: 0.9856\n",
      "Epoch 7/7\n",
      "414/414 [==============================] - 43s 105ms/step - loss: 0.0595 - accuracy: 0.9818 - val_loss: 0.0264 - val_accuracy: 0.9898\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 0.0385 - accuracy: 0.9856\n",
      "Accuracy: 98.561758\n",
      "loss: 0.038495\n",
      "Epoch 1/9\n",
      "414/414 [==============================] - 163s 391ms/step - loss: 0.8348 - accuracy: 0.6106 - val_loss: 0.4639 - val_accuracy: 0.8397\n",
      "Epoch 2/9\n",
      "414/414 [==============================] - 151s 366ms/step - loss: 0.4597 - accuracy: 0.8325 - val_loss: 0.2330 - val_accuracy: 0.9192\n",
      "Epoch 3/9\n",
      "414/414 [==============================] - 152s 367ms/step - loss: 0.2517 - accuracy: 0.9205 - val_loss: 0.1110 - val_accuracy: 0.9645\n",
      "Epoch 4/9\n",
      "414/414 [==============================] - 150s 363ms/step - loss: 0.1483 - accuracy: 0.9515 - val_loss: 0.0774 - val_accuracy: 0.9793\n",
      "Epoch 5/9\n",
      "414/414 [==============================] - 143s 346ms/step - loss: 0.1058 - accuracy: 0.9674 - val_loss: 0.0520 - val_accuracy: 0.9848\n",
      "Epoch 6/9\n",
      "414/414 [==============================] - 143s 346ms/step - loss: 0.0840 - accuracy: 0.9755 - val_loss: 0.0279 - val_accuracy: 0.9894\n",
      "Epoch 7/9\n",
      "414/414 [==============================] - 144s 347ms/step - loss: 0.0516 - accuracy: 0.9837 - val_loss: 0.0320 - val_accuracy: 0.9873\n",
      "Epoch 8/9\n",
      "414/414 [==============================] - 143s 346ms/step - loss: 0.0440 - accuracy: 0.9857 - val_loss: 0.0342 - val_accuracy: 0.9886\n",
      "Epoch 9/9\n",
      "414/414 [==============================] - 144s 348ms/step - loss: 0.0411 - accuracy: 0.9848 - val_loss: 0.0197 - val_accuracy: 0.9907\n",
      "60/60 [==============================] - 1s 8ms/step - loss: 0.0245 - accuracy: 0.9882\n",
      "Accuracy: 98.815566\n",
      "loss: 0.024459\n"
     ]
    }
   ],
   "source": [
    "# 필요한 모듈 임포트\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Conv1D, GlobalMaxPool1D, concatenate\n",
    "\n",
    "\n",
    "# 데이터 읽어오기\n",
    "train_file = \"../../chatbot/book_ex/ch6/chatbot_data.csv\"\n",
    "data = pd.read_csv(train_file, delimiter=',')\n",
    "features = data['Q'].tolist()\n",
    "labels = data['label'].tolist()\n",
    "\n",
    "\n",
    "# 단어 인덱스 시퀀스 벡터, 음절단위 분류\n",
    "corpus = [preprocessing.text.text_to_word_sequence(text) for text in features]\n",
    "#print(corpus[0:10])\n",
    "\n",
    "\n",
    "# 단어 토큰화??, 음절단위 분리 후 또하는 이유는?\n",
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "\n",
    "# 임베딩, 라벨 인코더\n",
    "sequences = tokenizer.texts_to_sequences(corpus)  # 총 단어에 각기 다른 숫자를 부여한 리스트 반환 --> 임베딩의 한 방법중 라벨 인코딩 같음\n",
    "word_index = tokenizer.word_index\n",
    "print(type(sequences))\n",
    "print(sequences[0:10])\n",
    "\n",
    "\n",
    "# 15개 단어 차원으로 패딩이 들어감\n",
    "MAX_SEQ_LEN = 15  # 단어 시퀀스 벡터 크기\n",
    "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')\n",
    "print(type(padded_seqs))\n",
    "print(padded_seqs[0:10])\n",
    "\n",
    "\n",
    "# 학습용, 검증용, 테스트용 데이터셋 생성 ➌\n",
    "# 학습셋:검증셋:테스트셋 = 7:2:1\n",
    "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, labels))\n",
    "ds = ds.shuffle(len(features))\n",
    "\n",
    "train_size = int(len(padded_seqs) * 0.7)\n",
    "val_size = int(len(padded_seqs) * 0.2)\n",
    "test_size = int(len(padded_seqs) * 0.1)\n",
    "\n",
    "train_ds = ds.take(train_size).batch(20)\n",
    "val_ds = ds.skip(train_size).take(val_size).batch(20)\n",
    "test_ds = ds.skip(train_size + val_size).take(test_size).batch(20)\n",
    "\n",
    "\n",
    "def define_Model_Fit(ks_list, n_fileters, embed_vs, num_epoch, modl_name):\n",
    " \n",
    "\n",
    "    # 하이퍼파라미터 설정\n",
    "    dropout_prob = 0.5\n",
    "    EMB_SIZE = embed_vs\n",
    "    EPOCH = num_epoch\n",
    "    VOCAB_SIZE = len(word_index) + 1  # 전체 단어 수\n",
    "\n",
    "    # CNN 모델 정의\n",
    "    input_layer = Input(shape=(MAX_SEQ_LEN,))\n",
    "    embedding_layer = Embedding(VOCAB_SIZE, EMB_SIZE, input_length=MAX_SEQ_LEN)(input_layer)\n",
    "    dropout_emb = Dropout(rate=dropout_prob)(embedding_layer)\n",
    "\n",
    "    conv1 = Conv1D(filters=n_fileters, kernel_size=ks_list[0], padding='valid', activation=tf.nn.relu)(dropout_emb)\n",
    "    pool1 = GlobalMaxPool1D()(conv1)\n",
    "    conv2 = Conv1D(filters=n_fileters, kernel_size=ks_list[1], padding='valid', activation=tf.nn.relu)(dropout_emb)\n",
    "    pool2 = GlobalMaxPool1D()(conv2)\n",
    "    conv3 = Conv1D(filters=n_fileters, kernel_size=ks_list[2], padding='valid', activation=tf.nn.relu)(dropout_emb)\n",
    "    pool3 = GlobalMaxPool1D()(conv3)\n",
    "\n",
    "    # 3, 4, 5- gram 이후 합치기\n",
    "    concat = concatenate([pool1, pool2, pool3])\n",
    "    hidden = Dense(128, activation=tf.nn.relu)(concat)\n",
    "    dropout_hidden = Dropout(rate=dropout_prob)(hidden)\n",
    "    logits = Dense(3, name='logits')(dropout_hidden)\n",
    "    predictions = Dense(3, activation=tf.nn.softmax)(logits)\n",
    "\n",
    "    # 모델 생성\n",
    "    model = Model(inputs=input_layer, outputs=predictions)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # 모델 학습\n",
    "    model.fit(train_ds, validation_data=val_ds, epochs=EPOCH, verbose=1)\n",
    "\n",
    "    # 모델 평가(테스트 데이터셋 이용)\n",
    "    loss, accuracy = model.evaluate(test_ds, verbose=1)\n",
    "    print('Accuracy: %f' % (accuracy * 100))\n",
    "    print('loss: %f' % (loss))\n",
    "\n",
    "    # 모델 저장\n",
    "    model.save('../MODL/'+modl_name)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    #exp1\n",
    "    ks_list = [5,6,7]\n",
    "    n_fileters =256\n",
    "    embed_vs = 256\n",
    "    num_epoch = 7\n",
    "    define_Model_Fit(ks_list, n_fileters, embed_vs, num_epoch, \"cnn_model_v2.h5\")\n",
    "\n",
    "    #exp2\n",
    "    ks_list = [7,8,9]\n",
    "    n_fileters = 512\n",
    "    embed_vs = 512\n",
    "    num_epoch = 9\n",
    "    define_Model_Fit(ks_list, n_fileters, embed_vs, num_epoch, \"cnn_model_v3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 0s 2ms/step\n",
      "37/37 [==============================] - 0s 5ms/step\n",
      "37/37 [==============================] - 1s 13ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98       529\n",
      "           1       1.00      0.96      0.98       359\n",
      "           2       0.99      0.98      0.98       294\n",
      "\n",
      "    accuracy                           0.98      1182\n",
      "   macro avg       0.98      0.98      0.98      1182\n",
      "weighted avg       0.98      0.98      0.98      1182\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       529\n",
      "           1       0.99      0.98      0.99       359\n",
      "           2       0.99      1.00      0.99       294\n",
      "\n",
      "    accuracy                           0.99      1182\n",
      "   macro avg       0.99      0.99      0.99      1182\n",
      "weighted avg       0.99      0.99      0.99      1182\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       529\n",
      "           1       1.00      0.98      0.99       359\n",
      "           2       1.00      1.00      1.00       294\n",
      "\n",
      "    accuracy                           0.99      1182\n",
      "   macro avg       0.99      0.99      0.99      1182\n",
      "weighted avg       0.99      0.99      0.99      1182\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, labels))\n",
    "ds = ds.shuffle(len(features))\n",
    "\n",
    "#print(padded_seqs)\n",
    "\n",
    "train_size = int(len(padded_seqs) * 0.7)\n",
    "val_size = int(len(padded_seqs) * 0.2)\n",
    "test_size = int(len(padded_seqs) * 0.1)\n",
    "\n",
    "train_ds = ds.take(train_size).batch(20)\n",
    "val_ds = ds.skip(train_size).take(val_size).batch(20)\n",
    "#test_ds = ds.skip(train_size + val_size).take(test_size).batch(20)\n",
    "test_ds = ds.skip(train_size + val_size).take(test_size)\n",
    "\n",
    "\n",
    "test_x = []\n",
    "test_y = []\n",
    "for i, (e1,e2) in enumerate(test_ds):\n",
    "    #print(i, '-'*50)\n",
    "    test_x.append(list(e1.numpy()))\n",
    "    test_y.append(e2.numpy())\n",
    "    #print(type(e1), e1.numpy())\n",
    "    #print(type(e2), e2.numpy())\n",
    "    #if i == 0: break\n",
    "\n",
    "#print(x)\n",
    "\n",
    "#print(test_ds.shape)\n",
    "# 감정 분류 CNN 모델 불러오기\n",
    "model1 = load_model('../MODL/cnn_model.h5')\n",
    "model2 = load_model('../MODL/cnn_model_v2.h5')\n",
    "model3 = load_model('../MODL/cnn_model_v3.h5')\n",
    "\n",
    "\n",
    "# 다양하게 test_ds 전체 사용\n",
    "#model1.evaluate(test_ds, verbose=2)   # loss & acc\n",
    "#model2.evaluate(test_ds, verbose=2)   # loss & acc\n",
    "#model3.evaluate(test_ds, verbose=2)   # loss & acc\n",
    "\n",
    "pred_test_y1 = np.argmax(model1.predict(tf.convert_to_tensor(test_x)), axis=1)\n",
    "pred_test_y2 = np.argmax(model2.predict(tf.convert_to_tensor(test_x)), axis=1)\n",
    "pred_test_y3 = np.argmax(model3.predict(tf.convert_to_tensor(test_x)), axis=1)\n",
    "\n",
    "print(classification_report(test_y, pred_test_y1))\n",
    "print(classification_report(test_y, pred_test_y2))\n",
    "print(classification_report(test_y, pred_test_y3))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
