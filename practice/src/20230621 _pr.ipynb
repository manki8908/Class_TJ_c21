{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 어절: 문장에서 발음의 단위\n",
    "  * ex) '우리는 오늘 동해로 간다' --> 4개 어절\n",
    "* 음절: 최소의 발음 단위\n",
    "  * ex) '우', '리', '는', '오', '늘', '동', '해', '로', '간', '다'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bow1:  ['년', '6월', '뉴턴', '선생님', '제안', '트리니티', '입학']\n",
      "bow2:  ['년', '6월', '뉴턴', '선생님', '제안', '대학교', '입학']\n",
      "bow3:  ['맛', '밥', '뉴턴', '선생', '님과 함께']\n",
      "n_gram_bow=  ('년', '6월', '뉴턴', '선생님', '제안', '트리니티', '입학')\n",
      "3\n",
      "n_gram_bow=  ('년', '6월', '뉴턴', '선생님', '제안', '대학교', '입학')\n",
      "3\n",
      "n_gram_bow=  ('맛', '밥', '뉴턴', '선생', '님과 함께')\n",
      "2\n",
      "n_gram out:  (('년', '6월'), ('6월', '뉴턴'), ('뉴턴', '선생님'), ('선생님', '제안'), ('제안', '트리니티'), ('트리니티', '입학'))\n",
      "n_gram out:  (('년', '6월'), ('6월', '뉴턴'), ('뉴턴', '선생님'), ('선생님', '제안'), ('제안', '대학교'), ('대학교', '입학'))\n",
      "n_gram out:  (('맛', '밥'), ('밥', '뉴턴'), ('뉴턴', '선생'), ('선생', '님과 함께'))\n",
      "유사도:  0.6666666666666666\n",
      "유사도:  0.0\n",
      "n_gram_bow=  ('년', '6월', '뉴턴', '선생님', '제안', '트리니티', '입학')\n",
      "2\n",
      "n_gram_bow=  ('년', '6월', '뉴턴', '선생님', '제안', '대학교', '입학')\n",
      "2\n",
      "n_gram_bow=  ('맛', '밥', '뉴턴', '선생', '님과 함께')\n",
      "1\n",
      "n_gram out:  (('년', '6월', '뉴턴'), ('6월', '뉴턴', '선생님'), ('뉴턴', '선생님', '제안'), ('선생님', '제안', '트리니티'), ('제안', '트리니티', '입학'))\n",
      "n_gram out:  (('년', '6월', '뉴턴'), ('6월', '뉴턴', '선생님'), ('뉴턴', '선생님', '제안'), ('선생님', '제안', '대학교'), ('제안', '대학교', '입학'))\n",
      "n_gram out:  (('맛', '밥', '뉴턴'), ('밥', '뉴턴', '선생'), ('뉴턴', '선생', '님과 함께'))\n",
      "유사도:  0.6\n",
      "유사도:  0.0\n",
      "n_gram_bow=  ('년', '6월', '뉴턴', '선생님', '제안', '트리니티', '입학')\n",
      "1\n",
      "n_gram_bow=  ('년', '6월', '뉴턴', '선생님', '제안', '대학교', '입학')\n",
      "1\n",
      "n_gram_bow=  ('맛', '밥', '뉴턴', '선생', '님과 함께')\n",
      "1\n",
      "n_gram out:  (('년', '6월', '뉴턴', '선생님'), ('6월', '뉴턴', '선생님', '제안'), ('뉴턴', '선생님', '제안', '트리니티'), ('선생님', '제안', '트리니티', '입학'))\n",
      "n_gram out:  (('년', '6월', '뉴턴', '선생님'), ('6월', '뉴턴', '선생님', '제안'), ('뉴턴', '선생님', '제안', '대학교'), ('선생님', '제안', '대학교', '입학'))\n",
      "n_gram out:  (('맛', '밥', '뉴턴', '선생'), ('밥', '뉴턴', '선생', '님과 함께'))\n",
      "유사도:  0.5\n",
      "유사도:  0.0\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Komoran\n",
    "\n",
    "# n_gram 유사도\n",
    "# 1. 명사 토큰화, 2. 토큰 n-gram화, 3. 비율세기\n",
    "\n",
    "# 어절 단위 n-gram\n",
    "def word_ngram(bow, num_gram):\n",
    "    text = tuple(bow)\n",
    "    print(\"n_gram_bow= \", text)\n",
    "    end_idx = len(text)//num_gram\n",
    "    print(end_idx)\n",
    "    ngrams = [text[x:x + num_gram] for x in range(0, len(text)-num_gram+1)]\n",
    "    #ngrams = [text[x:x + num_gram] for x in range(0, end_idx)]\n",
    "    return tuple(ngrams)\n",
    "\n",
    "\n",
    "# 음절 n-gram 분석\n",
    "def phoneme_ngram(bow, num_gram):\n",
    "    sentence = ' '.join(bow)\n",
    "    text = tuple(sentence)\n",
    "    slen = len(text)\n",
    "    ngrams = [text[x:x + num_gram] for x in range(0, slen)]\n",
    "    return ngrams\n",
    "\n",
    "\n",
    "# 유사도 계산\n",
    "def similarity(doc1, doc2):\n",
    "    cnt = 0\n",
    "    for token in doc1:\n",
    "        if token in doc2:\n",
    "            cnt = cnt + 1\n",
    "\n",
    "    return cnt/len(doc1)\n",
    "\n",
    "\n",
    "sentence1 = '1666년 6월에 뉴턴은 선생님의 제안으로 트리니티에 입학하였다'\n",
    "sentence2 = '1666년 6월에 뉴턴은 선생님의 제안으로 대학교에 입학하였다'\n",
    "sentence3 = '나는 맛잇는 밥을 뉴턴 선생님과 함께 먹었습니다.'\n",
    "\n",
    "komoran = Komoran()\n",
    "bow1 = komoran.nouns(sentence1)\n",
    "bow2 = komoran.nouns(sentence2)\n",
    "bow3 = komoran.nouns(sentence3)\n",
    "print(\"bow1: \", bow1)\n",
    "print(\"bow2: \", bow2)\n",
    "print(\"bow3: \", bow3)\n",
    "\n",
    "doc1 = word_ngram(bow1, 2)\n",
    "doc2 = word_ngram(bow2, 2)\n",
    "doc3 = word_ngram(bow3, 2)\n",
    "\n",
    "print(\"n_gram out: \", doc1)\n",
    "print(\"n_gram out: \", doc2)\n",
    "print(\"n_gram out: \", doc3)\n",
    "\n",
    "r1 = similarity(doc1, doc2)\n",
    "r2 = similarity(doc3, doc1)\n",
    "print(\"유사도: \", r1)\n",
    "print(\"유사도: \", r2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "doc1 = word_ngram(bow1, 3)\n",
    "doc2 = word_ngram(bow2, 3)\n",
    "doc3 = word_ngram(bow3, 3)\n",
    "\n",
    "print(\"n_gram out: \", doc1)\n",
    "print(\"n_gram out: \", doc2)\n",
    "print(\"n_gram out: \", doc3)\n",
    "\n",
    "r1 = similarity(doc1, doc2)\n",
    "r2 = similarity(doc3, doc1)\n",
    "print(\"유사도: \", r1)\n",
    "print(\"유사도: \", r2)\n",
    "\n",
    "\n",
    "\n",
    "doc1 = word_ngram(bow1, 4)\n",
    "doc2 = word_ngram(bow2, 4)\n",
    "doc3 = word_ngram(bow3, 4)\n",
    "\n",
    "print(\"n_gram out: \", doc1)\n",
    "print(\"n_gram out: \", doc2)\n",
    "print(\"n_gram out: \", doc3)\n",
    "\n",
    "r1 = similarity(doc1, doc2)\n",
    "r2 = similarity(doc3, doc1)\n",
    "print(\"유사도: \", r1)\n",
    "print(\"유사도: \", r2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[질문] 그림 5-2예시랑 코드 결과랑 마지막 성분이 다름, 코드는 마지막 성분이 unigram으로 하나 더 추가로 나옴\n",
    "--> len(text)-1 --> len(text) - num_n + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bow1:  ['집안일', '로봇', '집안일', '동영상', '분만', '터득']\n",
      "bow2:  ['집안일', '로봇', '스승', '동영상']\n",
      "n_gram out:  (('집안일', '로봇'), ('로봇', '집안일'), ('집안일', '동영상'), ('동영상', '분만'), ('분만', '터득'))\n",
      "n_gram out:  (('집안일', '로봇'), ('로봇', '스승'), ('스승', '동영상'))\n",
      "유사도:  0.2\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Komoran\n",
    "\n",
    "# n_gram 유사도\n",
    "# 1. 명사 토큰화, 2. 토큰 n-gram화, 3. 비율세기\n",
    "\n",
    "class n_gram():\n",
    "\n",
    "    # 어절 단위 n-gram\n",
    "    def word_ngram(bow, num_gram):\n",
    "        text = tuple(bow)\n",
    "        #print(\"n_gram_bow= \", text)\n",
    "        end_idx = len(text)//num_gram\n",
    "        #print(end_idx)\n",
    "        ngrams = [text[x:x + num_gram] for x in range(0, len(text)-num_gram+1)]\n",
    "        return tuple(ngrams)\n",
    "\n",
    "\n",
    "    # 음절 n-gram 분석\n",
    "    def phoneme_ngram(bow, num_gram):\n",
    "        sentence = ' '.join(bow)\n",
    "        text = tuple(sentence)\n",
    "        slen = len(text)\n",
    "        ngrams = [text[x:x + num_gram] for x in range(0, slen)]\n",
    "        return ngrams\n",
    "\n",
    "\n",
    "    # 유사도 계산\n",
    "    def similarity(doc1, doc2):\n",
    "        cnt = 0\n",
    "        for token in doc1:\n",
    "            if token in doc2:\n",
    "                cnt = cnt + 1\n",
    "\n",
    "        return cnt/len(doc1)\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "\n",
    "    while True:\n",
    "        sentence1 = input(\"3개 이상의 단어로 구성된 문장1을(를) 입력하세요\\n예:사람은 표유류에 속하는 동물이다\\n문장1 :\")\n",
    "        sentence2 = input(\"3개 이상의 단어로 구성된 문장2을(를) 입력하세요\\n예:사람은 설치류에 속하는 동물이다\\n문장2 :\")\n",
    "        if len(sentence1) > 0 and len(sentence2) > 0:\n",
    "            break\n",
    "    #sentence1 = input(\"문장1(기준)을 입력하세요: \")\n",
    "    #sentence2 = input(\"문장2(비교)를 입력하세요: \")\n",
    "  \n",
    "    komoran = Komoran()\n",
    "    bow1 = komoran.nouns(sentence1)\n",
    "    bow2 = komoran.nouns(sentence2)\n",
    "    print(\"bow1: \", bow1)\n",
    "    print(\"bow2: \", bow2)\n",
    "\n",
    "    doc1 = n_gram.word_ngram(bow1, 2)\n",
    "    doc2 = n_gram.word_ngram(bow2, 2)\n",
    "\n",
    "    print(\"n_gram out: \", doc1)\n",
    "    print(\"n_gram out: \", doc2)\n",
    "\n",
    "    r1 = n_gram.similarity(doc1, doc2)\n",
    "    print(\"유사도: \", r1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_gram out:  (('사람', '포유류'), ('포유류', '속'), ('속', '동물'))\n",
      "n_gram out:  (('사람', '설치류'), ('설치류', '속'), ('속', '동물'))\n",
      "n_gram out:  (('지우개', '필통'), ('필통', '물건'))\n",
      "n_gram out:  (('지우개', '필통'), ('필통', '학용품'), ('학용품', '이다'))\n",
      "n_gram out:  (('년', '6월 20일'), ('6월 20일', '저녁'), ('저녁', '오늘'), ('오늘', '새벽'), ('새벽', '비'), ('비', '기온'), ('기온', '이상'))\n",
      "n_gram out:  (('년', '6월 20일'), ('6월 20일', '저녁'), ('저녁', '오늘'), ('오늘', '새벽'), ('새벽', '비'))\n",
      "n_gram out:  (('집안일', '로봇'), ('로봇', '집안일'), ('집안일', '동영상'), ('동영상', '분만'), ('분만', '터득'))\n",
      "n_gram out:  (('집안일', '로봇'), ('로봇', '스승'), ('스승', '동영상'))\n",
      "유사도:  0.3333333333333333\n",
      "유사도:  0.5\n",
      "유사도:  0.7142857142857143\n",
      "유사도:  0.2\n"
     ]
    }
   ],
   "source": [
    "# sentence1 = \"사람은 포유류에 속하는 동물이다\"\n",
    "# sentence2 = \"사람은 설치류에 속하는 동물이다\"\n",
    "# sentence3 = \"지우개는 필통에 넣는 물건이다\"\n",
    "# sentence4 = \"지우개는 필통에 넣는 학용품이다\"\n",
    "# sentence5 = '2023년 6월 20일 늦은 저녁부터 오늘 새벽까지 비가 많이왔고, 기온도 3도 이상 떨어졌다'\n",
    "# sentence6 = '2023년 6월 20일 늦은 저녁부터 오늘 새벽까지 비가 많이 왔다'\n",
    "# sentence7 = '집안일 돕는 로봇은, 집안일 동영상을 보고 25분만에 터득했다'\n",
    "# sentence8 = \"집안일 돕는 로봇, 스승은 동영상\"\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bow:  ['6월', '뉴턴', '선생님', '제안', '트리니티', '입학', '6월', '뉴턴', '선생님', '제안', '대학교', '입학', '맛', '밥', '뉴턴', '선생', '님과 함께']\n",
      "word_dics:  ['6월', '뉴턴', '선생님', '제안', '트리니티', '입학', '대학교', '맛', '밥', '선생', '님과 함께']\n",
      "{'6월': 1, '뉴턴': 1, '선생님': 1, '제안': 1, '트리니티': 1, '입학': 1, '대학교': 0, '맛': 0, '밥': 0, '선생': 0, '님과 함께': 0}\n",
      "{'6월': 1, '뉴턴': 1, '선생님': 1, '제안': 1, '트리니티': 0, '입학': 1, '대학교': 1, '맛': 0, '밥': 0, '선생': 0, '님과 함께': 0}\n",
      "{'6월': 0, '뉴턴': 1, '선생님': 0, '제안': 0, '트리니티': 0, '입학': 0, '대학교': 0, '맛': 1, '밥': 1, '선생': 1, '님과 함께': 1}\n",
      "[1 1 1 1 1 1 0 0 0 0 0]\n",
      "[1 1 1 1 0 1 1 0 0 0 0]\n",
      "[0 1 0 0 0 0 0 1 1 1 1]\n",
      "0.8333333333333335\n",
      "0.18257418583505536\n"
     ]
    }
   ],
   "source": [
    "#예제-5-2-코사인 유사도 계산\n",
    "from konlpy.tag import Komoran\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "# 코사인 유사도 계산-1\n",
    "def cos_sim(vec1, vec2):\n",
    "    return dot(vec1, vec2) / (norm(vec1) * norm(vec2))\n",
    " \n",
    "\n",
    "# TDM 만들기-2\n",
    "# 전체 단어 셋트와 비교문장 단어 세트 비교해서 count\n",
    "def make_term_doc_mat(sentence_bow, word_dics):\n",
    "    freq_mat = {}\n",
    "    for word in word_dics:\n",
    "        freq_mat[word] = 0\n",
    "    for word in word_dics:\n",
    "        if word in sentence_bow:\n",
    "            freq_mat[word] += 1\n",
    "    return freq_mat\n",
    "\n",
    "\n",
    "# 단어 벡터 만들기-3, \n",
    "# make_term_doc_mat 여기서 만든 dict의 value값 삽입\n",
    "def make_vector(tdm):\n",
    "    vec = []\n",
    "    for key in tdm:\n",
    "        vec.append(tdm[key])\n",
    "    return vec\n",
    " \n",
    "# 문장 정의\n",
    "sentence1 = '6월에 뉴턴은 선생님의 제안으로 트리니티에 입학하였다'\n",
    "sentence2 = '6월에 뉴턴은 선생님의 제안으로 대학교에 입학하였다'\n",
    "sentence3 = '나는 맛잇는 밥을 뉴턴 선생님과 함께 먹었습니다.'\n",
    " \n",
    "# 헝태소분석기를 이용해 단어 묶음 리스트 생성-4\n",
    "komoran = Komoran()\n",
    "bow1 = komoran.nouns(sentence1)\n",
    "bow2 = komoran.nouns(sentence2)\n",
    "bow3 = komoran.nouns(sentence3)\n",
    " \n",
    "# 단어 묶음 리스트를 하나로 합침-5\n",
    "bow = bow1 + bow2 + bow3\n",
    "print(\"bow: \", bow)\n",
    " \n",
    "# 단어 묶음에서 중복제거해 단어 사전 구축-6\n",
    "word_dics = []\n",
    "for token in bow:\n",
    "    if token not in word_dics:\n",
    "        word_dics.append(token)\n",
    "print(\"word_dics: \", word_dics)\n",
    " \n",
    "# 문장 별 단어 문서 행렬 계산-7\n",
    "freq_list1 = make_term_doc_mat(bow1, word_dics)\n",
    "freq_list2 = make_term_doc_mat(bow2, word_dics)\n",
    "freq_list3 = make_term_doc_mat(bow3, word_dics)\n",
    "print(freq_list1)\n",
    "print(freq_list2)\n",
    "print(freq_list3)\n",
    " \n",
    "# 문장 백터 생성-8\n",
    "doc1 = np.array(make_vector(freq_list1))\n",
    "doc2 = np.array(make_vector(freq_list2))\n",
    "doc3 = np.array(make_vector(freq_list3))\n",
    "print(doc1)\n",
    "print(doc2)\n",
    "print(doc3)\n",
    " \n",
    "# 코사인 유사도 계산-9\n",
    "r1 = cos_sim(doc1, doc2)\n",
    "r2 = cos_sim(doc3, doc1)\n",
    "print(r1)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bow:  ['6월', '뉴턴', '선생님', '제안', '트리니티', '입학', '6월', '뉴턴', '선생님', '제안', '선생님', '대학교', '입학', '맛', '밥', '뉴턴', '선생', '님과 함께']\n",
      "word_dics:  ['6월', '뉴턴', '선생님', '제안', '트리니티', '입학', '대학교', '맛', '밥', '선생', '님과 함께']\n",
      "{'6월': 1, '뉴턴': 1, '선생님': 1, '제안': 1, '트리니티': 1, '입학': 1, '대학교': 0, '맛': 0, '밥': 0, '선생': 0, '님과 함께': 0}\n",
      "{'6월': 1, '뉴턴': 1, '선생님': 1, '제안': 1, '트리니티': 0, '입학': 1, '대학교': 1, '맛': 0, '밥': 0, '선생': 0, '님과 함께': 0}\n",
      "{'6월': 0, '뉴턴': 1, '선생님': 0, '제안': 0, '트리니티': 0, '입학': 0, '대학교': 0, '맛': 1, '밥': 1, '선생': 1, '님과 함께': 1}\n",
      "[1 1 1 1 1 1 0 0 0 0 0]\n",
      "[1 1 1 1 0 1 1 0 0 0 0]\n",
      "[0 1 0 0 0 0 0 1 1 1 1]\n",
      "0.8333333333333335\n",
      "0.18257418583505536\n"
     ]
    }
   ],
   "source": [
    "#예제-5-2-코사인 유사도 계산\n",
    "from konlpy.tag import Komoran\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "# 코사인 유사도 계산-1\n",
    "def cos_sim(vec1, vec2):\n",
    "    return dot(vec1, vec2) / (norm(vec1) * norm(vec2))\n",
    " \n",
    "\n",
    "# TDM 만들기-2\n",
    "# 전체 단어 셋트와 비교문장 단어 세트 비교해서 count\n",
    "def make_term_doc_mat(sentence_bow, word_dics):\n",
    "    freq_mat = {}\n",
    "    for word in word_dics:\n",
    "        freq_mat[word] = 0\n",
    "    for word in word_dics:\n",
    "        if word in sentence_bow:\n",
    "            freq_mat[word] += 1\n",
    "    return freq_mat\n",
    "\n",
    "\n",
    "# 단어 벡터 만들기-3, \n",
    "# make_term_doc_mat 여기서 만든 dict의 value값 삽입\n",
    "def make_vector(tdm):\n",
    "    vec = []\n",
    "    for key in tdm:\n",
    "        vec.append(tdm[key])\n",
    "    return vec\n",
    " \n",
    "# 문장 정의\n",
    "sentence1 = '6월에 뉴턴은 선생님의 제안으로 트리니티에 입학하였다'\n",
    "sentence2 = '6월에 뉴턴은 선생님의 제안으로 선생님의 대학교에 입학하였다'\n",
    "sentence3 = '나는 맛잇는 밥을 뉴턴 선생님과 함께 먹었습니다.'\n",
    " \n",
    "# 헝태소분석기를 이용해 단어 묶음 리스트 생성-4\n",
    "komoran = Komoran()\n",
    "bow1 = komoran.nouns(sentence1)\n",
    "bow2 = komoran.nouns(sentence2)\n",
    "bow3 = komoran.nouns(sentence3)\n",
    " \n",
    "# 단어 묶음 리스트를 하나로 합침-5\n",
    "bow = bow1 + bow2 + bow3\n",
    "print(\"bow: \", bow)\n",
    " \n",
    "# 단어 묶음에서 중복제거해 단어 사전 구축-6\n",
    "word_dics = []\n",
    "for token in bow:\n",
    "    if token not in word_dics:\n",
    "        word_dics.append(token)\n",
    "print(\"word_dics: \", word_dics)\n",
    " \n",
    "# 문장 별 단어 문서 행렬 계산-7\n",
    "freq_list1 = make_term_doc_mat(bow1, word_dics)\n",
    "freq_list2 = make_term_doc_mat(bow2, word_dics)\n",
    "freq_list3 = make_term_doc_mat(bow3, word_dics)\n",
    "print(freq_list1)\n",
    "print(freq_list2)\n",
    "print(freq_list3)\n",
    " \n",
    "# 문장 백터 생성-8\n",
    "doc1 = np.array(make_vector(freq_list1))\n",
    "doc2 = np.array(make_vector(freq_list2))\n",
    "doc3 = np.array(make_vector(freq_list3))\n",
    "print(doc1)\n",
    "print(doc2)\n",
    "print(doc3)\n",
    " \n",
    "# 코사인 유사도 계산-9\n",
    "r1 = cos_sim(doc1, doc2)\n",
    "r2 = cos_sim(doc3, doc1)\n",
    "print(r1)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 344 entries, 0 to 343\n",
      "Data columns (total 7 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   species            344 non-null    object \n",
      " 1   island             344 non-null    object \n",
      " 2   bill_length_mm     342 non-null    float64\n",
      " 3   bill_depth_mm      342 non-null    float64\n",
      " 4   flipper_length_mm  342 non-null    float64\n",
      " 5   body_mass_g        342 non-null    float64\n",
      " 6   sex                333 non-null    object \n",
      "dtypes: float64(4), object(3)\n",
      "memory usage: 18.9+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 데이터를 시각화 할 때 필요한 라이브러리들\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    " \n",
    "\n",
    "df = sns.load_dataset(\"penguins\")\n",
    "\n",
    "#일부 데이터만 보기\n",
    "\n",
    "df.head()\n",
    "\n",
    "# 데이터 길이\n",
    "\n",
    "len (df) # 344\n",
    "\n",
    " \n",
    "\n",
    "df.shape # (344, 7)\n",
    "\n",
    " \n",
    "\n",
    "# 요약정보 보기\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모든 문장 명사 추출:  ['6월', '뉴턴', '선생님', '제안', '트리니티', '입학', '6월', '뉴턴', '선생님', '제안', '선생님', '대학교', '입학', '맛', '밥', '뉴턴', '선생', '님과 함께']\n",
      "모든 문장 명사 중복 제거:  ['6월', '뉴턴', '선생님', '제안', '트리니티', '입학', '대학교', '맛', '밥', '선생', '님과 함께']\n",
      "문장1 명사 카운트 dict:  {'6월': 1, '뉴턴': 1, '선생님': 1, '제안': 1, '트리니티': 1, '입학': 1, '대학교': 0, '맛': 0, '밥': 0, '선생': 0, '님과 함께': 0}\n",
      "문장2 명사 카운트 dict:  {'6월': 1, '뉴턴': 1, '선생님': 1, '제안': 1, '트리니티': 0, '입학': 1, '대학교': 1, '맛': 0, '밥': 0, '선생': 0, '님과 함께': 0}\n",
      "문장3 명사 카운트 dict:  {'6월': 0, '뉴턴': 1, '선생님': 0, '제안': 0, '트리니티': 0, '입학': 0, '대학교': 0, '맛': 1, '밥': 1, '선생': 1, '님과 함께': 1}\n",
      "문장1 명사 벡터:  [1 1 1 1 1 1 0 0 0 0 0]\n",
      "문장2 명사 벡터:  [1 1 1 1 0 1 1 0 0 0 0]\n",
      "문장3 명사 벡터:  [0 1 0 0 0 0 0 1 1 1 1]\n",
      "문장1, 문장2의 코사인유사도:  0.8333333333333335\n",
      "문장3, 문장1의 코사인유사도:  0.18257418583505536\n"
     ]
    }
   ],
   "source": [
    "#예제-5-2-코사인 유사도 계산\n",
    "from konlpy.tag import Komoran\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "# 코사인 유사도 계산-1\n",
    "def cos_sim(vec1, vec2):\n",
    "    return dot(vec1, vec2) / (norm(vec1) * norm(vec2))\n",
    " \n",
    "# TDM 만들기-2\n",
    "# 전체 단어 셋트와 비교문장 단어 세트 비교해서 count\n",
    "def make_term_doc_mat(sentence_bow, word_dics):\n",
    "    freq_mat = {}\n",
    "    for word in word_dics:\n",
    "        freq_mat[word] = 0\n",
    "    for word in word_dics:\n",
    "        if word in sentence_bow:\n",
    "            freq_mat[word] += 1\n",
    "    return freq_mat\n",
    "\n",
    "\n",
    "# 단어 벡터 만들기-3, \n",
    "# make_term_doc_mat 여기서 만든 dict의 value값 삽입\n",
    "def make_vector(tdm):\n",
    "    vec = []\n",
    "    for key in tdm:\n",
    "        vec.append(tdm[key])\n",
    "    return vec\n",
    " \n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    while True:\n",
    "        sentence1 = input(\"3개 이상의 단어로 구성된 문장1을(를) 입력하세요\\n예:사람은 표유류에 속하는 동물이다\\n문장1 :\")\n",
    "        sentence2 = input(\"3개 이상의 단어로 구성된 문장2을(를) 입력하세요\\n예:사람은 설치류에 속하는 동물이다\\n문장2 :\")\n",
    "        sentence3 = input(\"3개 이상의 단어로 구성된 문장3을(를) 입력하세요\\n예:사람은 설치류에 속하는 동물이다\\n문장3 :\")\n",
    "        if len(sentence1) > 0 and len(sentence2) and len(sentence3) > 0:\n",
    "            break\n",
    "\n",
    "    # 문장 정의\n",
    "    #sentence1 = '6월에 뉴턴은 선생님의 제안으로 트리니티에 입학하였다'\n",
    "    #sentence2 = '6월에 뉴턴은 선생님의 제안으로 선생님의 대학교에 입학하였다'\n",
    "    #sentence3 = '나는 맛잇는 밥을 뉴턴 선생님과 함께 먹었습니다.'\n",
    "\n",
    "    # 헝태소분석기를 이용해 단어 묶음 리스트 생성-4\n",
    "    komoran = Komoran()\n",
    "    bow1 = komoran.nouns(sentence1)\n",
    "    bow2 = komoran.nouns(sentence2)\n",
    "    bow3 = komoran.nouns(sentence3)\n",
    "    \n",
    "    # 단어 묶음 리스트를 하나로 합침-5\n",
    "    bow = bow1 + bow2 + bow3\n",
    "    print(\"모든 문장 명사 추출: \", bow)\n",
    "    \n",
    "    # 단어 묶음에서 중복제거해 단어 사전 구축-6\n",
    "    word_dics = []\n",
    "    for token in bow:\n",
    "        if token not in word_dics:\n",
    "            word_dics.append(token)\n",
    "    print(\"모든 문장 명사 중복 제거: \", word_dics)\n",
    "    \n",
    "    # 문장 별 단어 문서 행렬 계산-7\n",
    "    freq_list1 = make_term_doc_mat(bow1, word_dics)\n",
    "    freq_list2 = make_term_doc_mat(bow2, word_dics)\n",
    "    freq_list3 = make_term_doc_mat(bow3, word_dics)\n",
    "    print(\"문장1 명사 카운트 dict: \", freq_list1)\n",
    "    print(\"문장2 명사 카운트 dict: \", freq_list2)\n",
    "    print(\"문장3 명사 카운트 dict: \", freq_list3)\n",
    "    \n",
    "    # 문장 백터 생성-8\n",
    "    doc1 = np.array(make_vector(freq_list1))\n",
    "    doc2 = np.array(make_vector(freq_list2))\n",
    "    doc3 = np.array(make_vector(freq_list3))\n",
    "    print(\"문장1 명사 벡터: \", doc1)\n",
    "    print(\"문장2 명사 벡터: \", doc2)\n",
    "    print(\"문장3 명사 벡터: \", doc3)\n",
    "    \n",
    "    # 코사인 유사도 계산-9\n",
    "    r1 = cos_sim(doc1, doc2)\n",
    "    r2 = cos_sim(doc3, doc1)\n",
    "    print(\"문장1, 문장2의 코사인유사도: \", r1)\n",
    "    print(\"문장3, 문장1의 코사인유사도: \", r2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
