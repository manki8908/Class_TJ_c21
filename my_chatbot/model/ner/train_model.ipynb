{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플 크기 : \n",
      " 72000\n",
      "0번 째 샘플 단어 시퀀스 : \n",
      " ['가락지빵', '주문', '하', '고', '싶', '어요']\n",
      "0번 째 샘플 bio 태그 : \n",
      " ['B_FOOD', 'O', 'O', 'O', 'O', 'O']\n",
      "마지막 샘플 단어 시퀀스 : \n",
      " ['남산근린공원', '소학리구간']\n",
      "마지막 샘플 bio 태그 : \n",
      " ['B_LC', 'B_LC']\n",
      "샘플 단어 시퀀스 최대 길이 : 168\n",
      "샘플 단어 시퀀스 평균 길이 : 7.885694444444445\n",
      "BIO 태그 사전 크기 : 11\n",
      "단어 사전 크기 : 17869\n",
      "x_train, y_train\n",
      "[[1, 1]]\n",
      "[[4, 4]]\n",
      "{1: 'O', 2: 'B_DT', 3: 'B_FOOD', 4: 'B_LC', 5: 'I', 6: 'B_OG', 7: 'I_LC', 8: 'B_PS', 9: 'NNP', 10: 'B_TI', 0: 'PAD'}\n",
      "학습 샘플 시퀀스 형상 :  (57600, 40)\n",
      "학습 샘플 레이블 형상 :  (57600, 40, 11)\n",
      "테스트 샘플 시퀀스 형상 :  (14400, 40)\n",
      "테스트 샘플 레이블 형상 :  (14400, 40, 11)\n",
      "450/450 [==============================] - 285s 618ms/step - loss: 0.1114 - accuracy: 0.9696\n",
      "450/450 [==============================] - 25s 54ms/step - loss: 0.0465 - accuracy: 0.9853\n",
      "평가 결과 :  0.9853366613388062\n",
      "450/450 [==============================] - 25s 53ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: I_LC seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          NP       1.00      1.00      1.00       282\n",
      "           _       0.57      0.43      0.49       631\n",
      "         _DT       0.99      1.00      0.99     13306\n",
      "       _FOOD       1.00      1.00      1.00     11665\n",
      "         _LC       0.96      0.96      0.96      4351\n",
      "         _OG       0.65      0.33      0.44       481\n",
      "         _PS       0.46      0.33      0.38       358\n",
      "         _TI       0.94      0.21      0.35        70\n",
      "\n",
      "   micro avg       0.98      0.96      0.97     31144\n",
      "   macro avg       0.82      0.66      0.70     31144\n",
      "weighted avg       0.97      0.96      0.96     31144\n",
      "\n",
      "F1-score: 96.9%\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,\"../../\")\n",
    "from utils.Preprocess import Preprocess\n",
    "\n",
    "\n",
    "# 학습 파일 불러오기\n",
    "def read_file(file_name):\n",
    "    sents = []\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for idx, l in enumerate(lines):\n",
    "            if l[0] == ';' and lines[idx + 1][0] == '$':\n",
    "                this_sent = []\n",
    "            elif l[0] == '$' and lines[idx - 1][0] == ';':\n",
    "                continue\n",
    "            elif l[0] == '\\n':\n",
    "                sents.append(this_sent)\n",
    "            else:\n",
    "                this_sent.append(tuple(l.split()))\n",
    "    return sents\n",
    "\n",
    "\n",
    "# 학습용 말뭉치 데이터를 불러옴\n",
    "#corpus = read_file('./ner_train.txt')\n",
    "corpus = read_file('./mtn_ner_train.txt')\n",
    "#print(corpus[0:2]) # shape = ( batch, sequence(순서,단어,태깅,BIO) )\n",
    "# [ [('1', '가락지빵', 'NNG', 'B_FOOD'), ('2', '주문', 'NNP', 'O'), ('3', '하', 'VV', 'O'), ('4', '고', 'EC', 'O'), ('5', '싶', 'VX', 'O'), ('6', '어요', 'EC', 'O')]\n",
    "#   [('1', '가락지빵', 'NNG', 'B_FOOD'), ('2', '먹', 'VV', 'O'), ('3', '고', 'EC', 'O'), ('4', '싶', 'VX', 'O'), ('5', '어요', 'EC', 'O')]\n",
    "# ]\n",
    "\n",
    "\n",
    "# 말뭉치 데이터에서 단어와 BIO 태그만 불러와 학습용 데이터셋 생성\n",
    "sentences, tags = [], []\n",
    "\n",
    "for t in corpus:\n",
    "    tagged_sentence = []\n",
    "    sentence, bio_tag = [], []\n",
    "\n",
    "    for w in t:\n",
    "        tagged_sentence.append((w[1], w[3]))\n",
    "        sentence.append(w[1])\n",
    "        bio_tag.append(w[3])\n",
    "    \n",
    "    sentences.append(sentence)\n",
    "    tags.append(bio_tag)\n",
    "\n",
    "\n",
    "print(\"샘플 크기 : \\n\", len(sentences))\n",
    "print(\"0번 째 샘플 단어 시퀀스 : \\n\", sentences[0])\n",
    "print(\"0번 째 샘플 bio 태그 : \\n\", tags[0])\n",
    "print(\"마지막 샘플 단어 시퀀스 : \\n\", sentences[-1])\n",
    "print(\"마지막 샘플 bio 태그 : \\n\", tags[-1])\n",
    "print(\"샘플 단어 시퀀스 최대 길이 :\", max(len(l) for l in sentences))\n",
    "print(\"샘플 단어 시퀀스 평균 길이 :\", (sum(map(len, sentences))/len(sentences)))\n",
    "\n",
    "# 단어사전 및 태그 사전 크기\n",
    "p = Preprocess(word2index_dic='../../train_tools/dict/chatbot_dict3.bin',\n",
    "               userdic='../../utils/user_dic.tsv')\n",
    "tag_tokenizer = preprocessing.text.Tokenizer(lower=False) # 태그 정보는 lower=False 소문자로 변환하지 않는다.\n",
    "tag_tokenizer.fit_on_texts(tags)\n",
    "\n",
    "# 사전 수정\n",
    "index_to_ner = tag_tokenizer.index_word # 시퀀스 인덱스를 NER로 변환 하기 위해 사용\n",
    "index_to_ner[0] = 'PAD'  # 패딩값의 예측값 키워드 추가\n",
    "print(index_to_ner)\n",
    "vocab_size = len(p.word_index) + 1\n",
    "tag_size = len(tag_tokenizer.word_index) + 1\n",
    "print(\"BIO 태그 사전 크기 :\", tag_size)\n",
    "print(\"단어 사전 크기 :\", vocab_size)\n",
    "\n",
    "# 학습용 단어 시퀀스 생성(라벨 인덱싱)\n",
    "x_train = [p.get_wordidx_sequence(sent) for sent in sentences]\n",
    "y_train = tag_tokenizer.texts_to_sequences(tags)\n",
    "\n",
    "print(\"x_train, y_train\")\n",
    "print(x_train[-2:-1])\n",
    "print(y_train[-2:-1])\n",
    "\n",
    "\n",
    "# 시퀀스 패딩 처리\n",
    "max_len = 40\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, padding='post', maxlen=max_len)\n",
    "y_train = preprocessing.sequence.pad_sequences(y_train, padding='post', maxlen=max_len)\n",
    "\n",
    "# 학습 데이터와 테스트 데이터를 8:2의 비율로 분리\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train,\n",
    "                                                    test_size=.2,\n",
    "                                                    random_state=1234)\n",
    "\n",
    "# 출력 데이터를 one-hot encoding\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=tag_size)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=tag_size)\n",
    "\n",
    "print(\"학습 샘플 시퀀스 형상 : \", x_train.shape)\n",
    "print(\"학습 샘플 레이블 형상 : \", y_train.shape)\n",
    "print(\"테스트 샘플 시퀀스 형상 : \", x_test.shape)\n",
    "print(\"테스트 샘플 레이블 형상 : \", y_test.shape)\n",
    "\n",
    "\n",
    "# 모델 정의 (Bi-LSTM)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# \n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=30, input_length=max_len, mask_zero=True))\n",
    "model.add(Bidirectional(LSTM(200, return_sequences=True, dropout=0.50, recurrent_dropout=0.25)))\n",
    "model.add(TimeDistributed(Dense(tag_size, activation='softmax')))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(0.01), metrics=['accuracy'])\n",
    "#model.fit(x_train, y_train, batch_size=128, epochs=10)\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=1)\n",
    "# \n",
    "print(\"평가 결과 : \", model.evaluate(x_test, y_test)[1])\n",
    "model.save('ner_model2.h5')\n",
    "# \n",
    "# \n",
    "# 시퀀스를 NER 태그로 변환\n",
    "def sequences_to_tag(sequences):  # 예측값을 index_to_ner를 사용하여 태깅 정보로 변경하는 함수.\n",
    "    result = []\n",
    "    for sequence in sequences:  # 전체 시퀀스로부터 시퀀스를 하나씩 꺼낸다.\n",
    "        temp = []\n",
    "        for pred in sequence:  # 시퀀스로부터 예측값을 하나씩 꺼낸다.\n",
    "            pred_index = np.argmax(pred)  # 예를 들어 [0, 0, 1, 0 ,0]라면 1의 인덱스인 2를 리턴한다.\n",
    "            temp.append(index_to_ner[pred_index].replace(\"PAD\", \"O\"))  # 'PAD'는 'O'로 변경\n",
    "        result.append(temp)\n",
    "    return result\n",
    "# \n",
    "# \n",
    "# f1 스코어 계산을 위해 사용\n",
    "from seqeval.metrics import f1_score, classification_report\n",
    "# \n",
    "# 테스트 데이터셋의 NER 예측\n",
    "y_predicted = model.predict(x_test)\n",
    "pred_tags = sequences_to_tag(y_predicted) # 예측된 NER\n",
    "test_tags = sequences_to_tag(y_test)    # 실제 NER\n",
    "# \n",
    "# F1 평가 결과\n",
    "print(classification_report(test_tags, pred_tags))\n",
    "print(\"F1-score: {:.1%}\".format(f1_score(test_tags, pred_tags)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
